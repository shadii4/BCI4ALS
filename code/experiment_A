# -*- coding: utf-8 -*-
"""
Created on Sat Nov 13 19:18:32 2021

@author: Shadii4
"""
# Setup
import scipy.io as spio
import numpy as np
import math
import seaborn as sns
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.datasets import make_blobs
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import ShuffleSplit
from sklearn.utils import shuffle
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neighbors import NearestNeighbors
from sklearn.neighbors import RadiusNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn import svm
import os
import sklearn
import torch
import matplotlib.pyplot as plt
from typing import Sequence
import torch.optim
import torch.nn as nn
import torch.utils.data
from torch.utils.data import TensorDataset, DataLoader

torch.manual_seed(42)
np.random.seed(42)

N_NEIGHBORS = 5 #choose this in parameter tuning
N_DAYS = 5 #number of online data records not including offline
KL_TAU = 0.3 #threshold for KL divergence

##Deep netowrk class - to create: MLP(D_in=10, hidden_dims=[128,64,16], D_out=2, nonlin='relu')
class MLP(torch.nn.Module):
    NLS = {'relu': torch.nn.ReLU, 'tanh': nn.Tanh, 'sigmoid': nn.Sigmoid, 'softmax': nn.Softmax, 'logsoftmax': nn.LogSoftmax, 'lrelu':nn.LeakyReLU}

    def __init__(self, D_in: int, hidden_dims: Sequence[int], D_out: int, nonlin='relu'):
        super().__init__()
        
        all_dims = [D_in, *hidden_dims, D_out]
        non_linearity = MLP.NLS[nonlin]
        layers = []
        
        for in_dim, out_dim in zip(all_dims[:-1], all_dims[1:]):
            layers += [
                nn.Linear(in_dim, out_dim, bias=True),
                non_linearity()
            ]
        
        # Sequential is a container for layers
        self.fc_layers = nn.Sequential(*layers[:-1])
        
        # Output non-linearity
        self.softmax = nn.Softmax(dim=1)

    def forward(self, x):
        x = torch.reshape(x, (x.shape[0], -1))
        z = self.fc_layers(x)
        
        y_pred = self.softmax(z)
        # Output is always probability
        return y_pred


def classifyOfflineData(X,y):
    #classify data and return the offline classifier
    clf = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)
    clf.fit(X,y)
    y_pred = clf.predict(X)
    tp = (y==y_pred) #tp is the accuracy
    print("Offline Accuracy: {:.2f}".format(tp.sum()/len(tp)))
    ##print("\n")
    return clf

def classifyData(X_array,y_array):
    ##classify data using new clf and return results
    results = []
    clf = KNeighborsClassifier(n_neighbors=N_NEIGHBORS)
    for i in range(len(y_array)):
        clf.fit(X_array[i], y_array[i])
        y_pred = clf.predict(X_array[i]) #should be split to train/test
        tp = (y_array[i]==y_pred) 
        results.append(tp.sum()/len(tp))
    return np.array(results)

def classifyBasedOnOffline_classifier(X_array,y_array,num_of_days,clf):
    ##classify using the given trained clf
    result = []
    for i in range(num_of_days):
        y_pred = clf.predict(X_array[i])
        tp = y_array[i] == y_pred #true predictions
        result.append(round(tp.sum()/len(tp),2))
    return np.array(result)

def sampleTheStarters(X_array,y_array,day,label,n_samples):
    #each new day we have n_smaples labeled 
    samples = [X_array[day][y_array[day]==label][0]]
    #WE should delete the samples taken!
    for i in np.arange(1,n_samples):
        samples = np.append(samples, [X_array[day][y_array[day]==label][i]],axis=0)
    return samples

def pre_online(X_array,y_array,day,X_offline,y_offline,d,n_added_samples,radius):
    ##construct the training set for a specific day 
    ##look at the presentation for further explanation
    n_samples_starters = 5 #how many samples to take in new day of each label
    center = [np.full(d,0)] #will delete this first row later - only for initialization
    var = [np.full(d,0)]    #same
    samples=[0]*(num_of_classes+1) #for each label. first cell is dummy
    for label in np.arange(1,num_of_classes+1):
        #sample the starters 
        samples[label] = sampleTheStarters(X_array,y_array,day,label,n_samples_starters) #the starters
        #compute mean
        center = np.append(center, [samples[label].mean(0)],axis=0)
        var = np.append(var, [np.full(d,2)],axis=0) #change it!!!!
    var = np.delete(var,0,0)
    center = np.delete(center, 0, 0)
    #generate samples around the starters
    X_Tr_t , y_Tr_t = make_blobs(n_samples=n_added_samples*num_of_classes, n_features=d, centers=center, cluster_std=var,shuffle=True, random_state=42, return_centers=False)
    y_Tr_t += 1
    for label in np.arange(1,num_of_classes+1):
        #add the real starters samples
        #samples = samples_labeled(X_array,y_array,day,label,n_samples_starters) #the starters
        X_Tr_t = np.append(X_Tr_t, samples[label],axis=0)
        for i in range(n_samples_starters):
            y_Tr_t = np.append(y_Tr_t,[label],axis=0)
           
       
        #add samples from offline data within given radius
        knn = NearestNeighbors(radius=radius)
        A = X_offline[y_offline==label] #points of label
        knn.fit(A)
        #samples_in_radius
        nearest_samples = knn.radius_neighbors([center[label-1]],return_distance=False) #find nearest neighbors to the center of the starters
        
        #now add them to the base point set
        #print('added : ',A[nearest_samples[0]])
        X_Tr_t = np.append(X_Tr_t,A[nearest_samples[0]],axis=0)
        for i in range(len(nearest_samples[0])):
            y_Tr_t = np.append(y_Tr_t,[label],axis=0)
            
    
    #add the starters samples to the offline data       
    for label in np.arange(1,num_of_classes+1): #might delete this for
        X_offline = np.append(X_offline, samples[label],axis=0)
        for i in range(n_samples_starters):
            y_offline = np.append(y_offline,[label],axis=0)
    return X_Tr_t,y_Tr_t , X_offline, y_offline


def onlineClassification(X_online_array,y_online_array,X_offline,y_offline,num_of_days,clf,d,n_added_samples,radius,CR_threshold,isDeep,mlp_model):
    ##online classification phase 
    result = []
    #accuracy_of_added_pts , c = 0 , 0
    m ,epoch = 0 , 8 #counter for added new points , #bound for new points to upgrade the clf
    for day in range(num_of_days):
        m=0
        #each day we update the offline data
        X_Tr , y_Tr , X_offline, y_offline = pre_online(X_online_array,y_online_array,day,X_offline, y_offline,d,n_added_samples,radius)
        #print("base shape:"+str(X_Tr.shape))
        if (not isDeep):
            clf.fit(X_Tr,y_Tr)
            tp = []
            #now we do the online test
            for sample,label in zip(X_online_array[day],y_online_array[day]):
                prediction = clf.predict([sample])
                #y_array_pred[i] = np.append(y_array_pred[i],prediction)
                tp = np.append(tp,prediction==label)
                if(np.any(clf.predict_proba([sample]) >= CR_threshold)):
                    #c=c+1
                    #if(prediction==label):
                        #accuracy_of_added_pts += 1.0
                    X_Tr = np.append(X_Tr, [sample],axis=0) #add it to the current training set
                    y_Tr = np.append(y_Tr, clf.predict([sample]),axis=0)
                    #add it to the global training set
                    X_offline = np.append(X_offline, [sample],axis=0) 
                    y_offline = np.append(y_offline, clf.predict([sample]),axis=0)
                    
                    m=m+1
                    if(m==epoch):
                        m=0
                        clf.fit(X_Tr,y_Tr)
        else: #IS DEEEEP
            n_features = X_Tr.shape[1]
            print('Xtr n features',n_features)
            n_classes = num_of_classes
            #train dataset
            tr_batch_size=16
            X_train_tensor = torch.tensor(X_Tr,dtype=torch.float)
            y_train_tensor = torch.tensor(y_Tr)
            train_size = X_Tr.shape[0]
            
            train_dataset = TensorDataset(X_train_tensor,y_train_tensor) # create your datset
            dl_train = DataLoader(train_dataset, tr_batch_size,
                                                   sampler=torch.utils.data.SubsetRandomSampler(range(0,train_size)))
            #test dataset
            ts_batch_size = 1
            X_test_tensor = torch.tensor(X_online_array[day],dtype=torch.float)
            y_test_tensor = torch.tensor(y_online_array[day])
            test_size = X_online_array[day].shape[0]
            test_dataset = TensorDataset(X_test_tensor,y_test_tensor) # create your datset
            dl_test = torch.utils.data.DataLoader(test_dataset, ts_batch_size,
                                                   sampler=torch.utils.data.SubsetRandomSampler(range(0,test_size)))
            
    
            ##Loss function
            loss_fn = nn.CrossEntropyLoss()
            
            # Optimizer over our model's parameters
            optimizer = torch.optim.SGD(params=mlp_model.parameters(), lr=0.01,weight_decay=0.001,momentum=0.4)
            #optimizer = torch.optim.Adam(params=model.parameters(), lr=0.01, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.001, amsgrad=False)
            num_epochs = 20
            for_every = 5
            c=0
            for epoch_idx in range(num_epochs):
                total_loss = 0
                c+=1
                for batch_idx, (X, y) in enumerate(dl_train):
                    # Forward pass
                    y_pred = mlp_model(X)
                    #print(y_pred)
                    # Compute loss
                    y_true = y.type(torch.LongTensor).detach() - 1
                    #print(y_pred,y_true)
                    loss = loss_fn(y_pred, y_true)
                    total_loss += loss.item()
            
                    # Backward pass
                    optimizer.zero_grad() # Zero gradients of all parameters
                    loss.backward()       # Run backprop algorithms to calculate gradients
                    
                    # Optimization step
                    optimizer.step()      # Use gradients to update model parameters
                if c%for_every==0:    
                    print(f'Epoch #{epoch_idx+1}: Avg. loss={total_loss/len(dl_train.dataset)}')           
            tp = []
            #now we do the online test
            X_retrain = torch.tensor([])
            y_retrain = torch.tensor([])
            for idx, (X, y) in enumerate(dl_test):#batch size is 1
                
                with torch.no_grad():
                    y_pred = mlp_model(X)
                prediction = torch.argmax(y_pred,axis=1) + 1 #prediction is in [1:3]
                pred_probability = prediction.max().item()
                tp = np.append(tp,prediction==y)
                if(pred_probability >= CR_threshold):
                    #c=c+1
                    #if(prediction==label):
                        #accuracy_of_added_pts += 1.0
                    X_Tr = np.append(X_Tr, [np.array(X[0])],axis=0) #add it to the current training set
                    y_Tr = np.append(y_Tr, [prediction],axis=0)
                    #add it to the global training set
                    X_offline = np.append(X_offline, [np.array(X[0])],axis=0) 
                    y_offline = np.append(y_offline, [prediction],axis=0)
                    
                    m=m+1
                    X_retrain = torch.cat((X_retrain, X))
                    y_retrain = torch.cat((y_retrain, y))
                    if(m==epoch):
                        # Forward pass
                        y_pred = mlp_model(X_retrain)
                        # Compute loss
                        y_true = y_retrain.type(torch.LongTensor) - 1
                        loss = loss_fn(y_pred, y_true)
                        total_loss += loss.item()
                
                        # Backward pass
                        optimizer.zero_grad() # Zero gradients of all parameters
                        loss.backward()       # Run backprop algorithms to calculate gradients
                        
                        # Optimization step
                        optimizer.step()      # Use gradients to update model parameters
                        X_retrain = torch.tensor([])
                        y_retrain = torch.tensor([])

                        m=0

            #if(i!=0):
            #X_array[i] = X_Tr #now current day is the offline data for next day
            #y_array[i] = y_Tr #now current day is the offline data for next day
        #print("accuracy of added pts",accuracy_of_added_pts/c)
        #print("Day "+str(i)+" Online ACC = {:.2f}".format((tp.sum()/len(tp))))
        #print("number of Samples collected : "+str(X_Tr.shape[0])+"\n")
        result.append(round(tp.sum()/len(tp),2))
    return np.array(result)
"""--------------------------------Loading Data---------------------------------------"""
"""Loading offline data"""
#MIFeature.mat , trainingVec.mat should in the directory file
#if there is Excel table. use read_csv instead
X_offline = spio.loadmat('MIFeatures_Offline.mat',squeeze_me=True)['MIFeatures']
y_offline = spio.loadmat('trainingVec_Offline.mat',squeeze_me=True)['trainingVec']
num_of_classes = len(np.unique(y_offline))
num_of_trials = X_offline.shape[0]

"""Loading online data"""
num_of_records = 5
num_of_days = N_DAYS
X_online_array , y_online_array = [] , []
X_online_records , y_online_records = [] , []

for i in range(num_of_records):
    data_filename = 'MIFeatures_Online' + str(i+1) + '.mat'
    labels_filename = 'trainingVec_Online' + str(i+1) + '.mat'
    X_online_records.append(spio.loadmat(data_filename, squeeze_me=True)['MIFeatures'])
    y_online_records.append(spio.loadmat(labels_filename, squeeze_me=True)['trainingVec'])
"""concatenating records at same day - change according to the specific data"""
#Day 0 (offline)
X_online_array.append(X_offline)
y_online_array.append(y_offline)
#DAY 1
X_online_array.append(X_online_records[0])
y_online_array.append(y_online_records[0])

#DAY 2
X_online_array.append(X_online_records[1])
y_online_array.append(y_online_records[1])

#DAY 3                      
X_online_array.append(X_online_records[2])
y_online_array.append(y_online_records[2])

#Day 4
X_online_array.append(np.concatenate((X_online_records[3],X_online_records[4]),axis=0))
y_online_array.append(np.concatenate((y_online_records[3],y_online_records[4]), axis=0))
"""-----------------------------------------------------------------------"""

"""---------------------------------CLEANING DATA------------------------------"""
X_df_array = []
sum_r=0
X_df_array.append(pd.DataFrame(np.concatenate((X_offline.copy(),y_offline.copy().reshape(-1,1)),axis=1)))
for day in range(num_of_days):
    X_df_array.append(pd.DataFrame(np.concatenate((X_online_array[day].copy(),y_online_array[day].copy().reshape(-1,1)),axis=1)))
"""Removing Outliers"""
for col in range(X_offline.shape[1]):
    for day in range(num_of_days+1):
        df_col = X_df_array[day].iloc[:,col].copy() #not sure if without the copy it is by refernce?
        first_shape = df_col.shape[0]
        Q1,Q3 = np.nanpercentile(sorted(df_col),[20,80])
        IQR = Q3-Q1
        low = Q1 - (1.5*IQR)
        high = Q3 + (1.5*IQR)
        df_col[df_col<low] = low - np.random.randn()
        df_col[df_col>high] = high + np.random.randn()
        X_df_array[day].iloc[:,col] = df_col.copy()
        last_shape = df_col.shape[0]
selected_stationary_features = [range(340)]

"""
groupBy=[]  
fig = plt.figure(figsize=(10,10))
for col in range(18):
    ax = fig.add_subplot(6,3,col+1)
    groupBy.append(X_df_array[0][[col,588]].groupby(588))
    groupBy[col][col].plot(kind='kde')
        #sum_r+=first_shape - last_shape
"""

"""Scaling"""
"""
scaler = StandardScaler() # x-mio/sigma
for day in range(num_of_days+1):
    X_df_array[day] = scaler.fit_transform(X_df_array[day])
"""


"""-------------------------------------------------------------------------"""
"""from pandas to np array"""
X_offline = X_df_array[0][selected_stationary_features[0]].values
for day in range(num_of_days):
    X_online_array[day] = X_df_array[day+1][selected_stationary_features[0]].values



""" our algorithm test"""
d = X_offline.shape[1]
print('num_features = ',d)

clf_offline = classifyOfflineData(X_offline,y_offline)
#initializations 
isDeep = False
mlp_model = []
results_based_on_offline_clf, results_based_on_our_algorithem = np.zeros((len(classifiers),num_of_days)),np.zeros((len(classifiers),num_of_days))
results_based_clf_each_day = []

#results based on new classifier in each day - OPTIMAL
results_based_clf_each_day = classifyData(X_online_array, y_online_array)
print('classifier performance based on clf each day: ',results_based_clf_each_day)

#based on offline classifier - NAIVE
results_based_on_offline_clf = classifyBasedOnOffline_classifier(X_online_array,y_online_array,num_of_days,clf_offline)
print('classifier performance based on the offline data: ',results_based_on_offline_clf)
#our alg compared to naive
difference_between_results = np.zeros((len(classifiers),num_of_days))
#choose the hyper-parameters 
n_added_synthetic_samples = 20
radius = 20
CR_threshold = 0.7
#PLOT args
"""-----------------------------------KL-DIVERGENCE calculation and filtering------------------------"""
"""
def kl_divergence(p, q):
    e = 0.00001
    return sum((p[i]+e) * math.log((p[i]+e)/(q[i]+e)) for i in range(len(p)))
KL_Divergence = np.zeros((X_offline.shape[1]))
df1 = X_df_array[0]
df2 = X_df_array[3]
df3 = X_df_array[2]
for col in range(340): #without the label -1
    df_col1 = df1.iloc[:,col]
    df_col2 = df2.iloc[:,col]
    df_col3 = df2.iloc[:,col]
    
    df_col1_class1 = df_col1[df1[len(df1.columns)-1]==1]
    df_col1_class2 = df_col1[df1[len(df1.columns)-1]==2]
    df_col1_class3 = df_col1[df1[len(df1.columns)-1]==3]
    
    df_col2_class1 = df_col2[df2[len(df2.columns)-1]==1]
    df_col2_class2 = df_col2[df2[len(df2.columns)-1]==2]
    df_col2_class3 = df_col2[df2[len(df2.columns)-1]==3]
    
    df_col3_class1 = df_col3[df2[len(df3.columns)-1]==1]
    df_col3_class2 = df_col3[df2[len(df3.columns)-1]==2]
    df_col3_class3 = df_col3[df2[len(df3.columns)-1]==3]
    
    col1_class1_p = plt.hist(df_col1_class1)[0]/len(df_col1_class1)
    col2_class1_q = plt.hist(df_col2_class1)[0]/len(df_col2_class1)
    col3_class1_w = plt.hist(df_col3_class1)[0]/len(df_col3_class1)
     
    col1_class2_p = plt.hist(df_col1_class2)[0]/len(df_col1_class2)
    col2_class2_q = plt.hist(df_col2_class2)[0]/len(df_col2_class2)
    col3_class2_w = plt.hist(df_col3_class2)[0]/len(df_col3_class2)
    
    col1_class3_p = plt.hist(df_col1_class3)[0]/len(df_col1_class3)
    col2_class3_q = plt.hist(df_col2_class3)[0]/len(df_col2_class3)
    col3_class3_w = plt.hist(df_col3_class3)[0]/len(df_col3_class3)
    
    min_value_between12 = [kl_divergence(col1_class1_p,col2_class1_q),
                          kl_divergence(col1_class2_p,col2_class2_q),
                          kl_divergence(col1_class3_p,col2_class3_q)]
    min_value_between12 = min(min_value_between12)
    min_value_between13 = min(kl_divergence(col3_class1_w,col1_class1_p),
                          kl_divergence(col3_class2_w,col1_class1_p),
                          kl_divergence(col3_class3_w,col1_class1_p))
    
    if (min_value_between12>=0) and (min_value_between12<KL_TAU) and (min_value_between13>=0) and (min_value_between13<KL_TAU):
        KL_Divergence[col] = 1
selected_stationary_features = np.where(KL_Divergence==1)
d = len(selected_stationary_features[0])

"""
print('number of selected features',len(selected_stationary_features[0]))
X_offline = X_offline[:,selected_stationary_features[0]]
for day in range(num_of_days):
    X_online_array[day] = X_online_array[day][:,selected_stationary_features[0]]

## our clfs
classifiers_names = ['PWKNN','Naive Bayes','Decision Tree','Neural Network']
classifiers = [KNeighborsClassifier(n_neighbors=20,weights='distance'),GaussianNB(),DecisionTreeClassifier(max_depth=5),MLP(D_in=d, hidden_dims=[128], D_out=3, nonlin='relu')]
## ax1 for clfs comparing --ax2 for algs comparing using the first clf PWKNN
figure1,ax = plt.subplots()
bar_index = np.arange(num_of_days)
bar_width = 0.18
colors = ['red','blue','green','brown','purple']
for i , clf in enumerate(classifiers):        
    online_clf = clf
    if clf==classifiers[-1]:
        isDeep=True
        mlp_model = clf
    results_based_on_our_algorithem[i] = onlineClassification(X_online_array,y_online_array,X_offline,y_offline,num_of_days,online_clf,d,n_added_synthetic_samples,radius,CR_threshold,isDeep,mlp_model)
    ax.bar(bar_index+i*bar_width,results_based_on_our_algorithem[i],width=bar_width,color=colors[i],label=classifiers_names[i])
    

days_list = ['0','1','2','3','4']
ax.set_xlabel('Days')
ax.set_ylabel('Accuracy')
ax.set_yticks(np.arange(0,1.1,0.1))
ax.set_xticks(bar_index + bar_width*3/2)
ax.set_xticklabels(days_list)
ax.legend()
plt.show()

## to show KL divergence results this re run with uncommenting the KL-Divergence calculation and filtering
results_based_on_our_algorithem_kl = results_based_on_our_algorithem[0].copy()

figure2 , ax2 = plt.subplots()
ax2.plot(days_list,results_based_clf_each_day,color='green',label='Optimal Algorithm',marker='o')
ax2.plot(days_list,results_based_on_offline_clf,color='blue',label='Naive Algorithm',marker='o')
ax2.plot(days_list,results_based_on_our_algorithem[0],color='red',label='Our Algorithm',marker='o')
#ax2.plot(days_list,results_based_on_our_algorithem_kl,color='red',label='Our Algorithm + KL Divergence',marker='o',linestyle='dashed')
ax2.set_xlabel('Days')
ax2.set_ylabel('Accuracy')
ax2.set_xticklabels(days_list)
ax2.set_yticks(np.arange(0,1.1,0.1))
ax2.legend()
plt.show()

